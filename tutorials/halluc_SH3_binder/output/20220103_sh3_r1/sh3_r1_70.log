/mnt/home/dzorine/software/homog/homog/homog.py:98: SyntaxWarning: "is" with a literal. Did you mean "=="?
  if degrees is 'auto': degrees = guess_is_degrees(angle)
[21:29:41] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: libtorch_cuda_cpp.so: cannot open shared object file: No such file or directory
Using backend: pytorch
--steps was given. Ignoring --grad_steps, --mcmc_steps.

Run settings:
{'network_name': 'rf_Nov05_2021', 'use_template': None, 'num': 2, 'start_num': 70, 'msa_num': 1, 'out': '/mnt/home/jue/halluc/linear_motifs/output/20220103_sh3_r1/sh3_r1', 'cautious': 1, 'save_pdb': 1, 'save_batch_fas': False, 'track_step': 10, 'track_logits': False, 'out_step': None, 'seed_rng': False, 'steps': 'g600', 'grad_steps': 400, 'mcmc_steps': 0, 'optimizer': 'nsgd', 'drop': 0.2, 'init_sd': 1e-06, 'learning_rate': 0.05, 'grad_check': True, 'logit_scale': 1, 'seq_prob_type': 'hard', 'seq_sample': False, 'calc_bkg': True, 'cce_sd': None, 'hal_sd': None, 'corrupt_sequence': None, 'corrupt_fraction': None, 'pdb': '/mnt/home/jue/halluc/linear_motifs/input/SH3_2w0z.pdb', 'mask': None, 'contigs': 'B7-14', 'con_set_id': None, 'len': '55-100', 'keep_order': False, 'contig_min_gap': 5, 'spike': None, 'spike_fas': None, 'force_aa': 'B7-14', 'exclude_aa': 'C', 'force_aa_hal': None, 'template_pdbs': None, 'no_bkg_mask': False, 'num_repeats': 0, 'init_seq': None, 'masks_bkg': None, 'masks_pass': None, 'force_logits': None, 'receptor': None, 'rec_placement': 'second', 'gap': 200, 'w_cce': 1, 'w_crmsd': -1, 'w_entropy': 1, 'w_kl': -1, 'n_bkg': 100, 'w_rep': 2.0, 'w_set_rep': -1, 'w_atr': -1, 'w_set_atr': -1, 'w_rog': 1.0, 'w_aspect_ratio': -1, 'w_cyc_sym': -1, 'w_surfnp': -1, 'w_nc': -1, 'w_cce_bg': -1, 'w_sym': -1, 'cce_cutoff': 19.9, 'rep_pdb': 'input/SH3_2w0z_rec.pdb', 'rep_sigma': 3.5, 'atr_pdb': None, 'atr_sigma': 5, 'entropy_beta': 10, 'rog_thresh': 16.0, 'surfnp_nbr_thresh': 2.5, 'nc_target': -7, 'entropy_dist_bins': 16, 'mcmc_halflife': 500.0, 'T_acc_0': 0.002, 'mcmc_batch': 1, 'anneal_t1d': False, 'erode_template': False, 'num_masked_tokens': 1, 'weights_dir': '/projects/ml/trDesign', 'nthreads': 4, 'cce_cutstep': None, 'cce_thresh': 2.2, 'batch': 64, 'lr': 0.2, 'nsteps': 100, 'commit': 'c344913efafbbfe8f452574b0c86c348792a5045'}

Loading structure prediction model onto device cuda:0...
#   trunk_msa_v00     [ens=1]   AF2-inspired 12-block 2-track trunk
#   trunk_tbm_v00     [ens=1]   AF2-inspired 3-track trunk
#   rf_v00            [ens=1]   RoseTTAFold 3-track trunk + refiner (formerly trunk_e2e_v00)
# * rf_Nov05_2021     [ens=1]   RoseTTAFold 3-track, no perceiver, Nov. 5 2021
#   rf_perceiver_v00  [ens=1]   RoseTTAFold w/ Perceiver & FAPE loss (msa_full=zeros)
#   rf_perceiver_v01  [ens=1]   RoseTTAFold w/ Perceiver & FAPE loss (msa_full=msa_latent)
#   af2_v00           [ens=0]   AlphaFold2 (only works with rescue.py)
Loaded sequence-to-structure model rf_Nov05_2021 with 66037142 parameters

Model hyperparameters:
{'SE3_param': {'div': 4, 'l0_in_features': 32, 'l0_out_features': 32, 'l1_in_features': 3, 'l1_out_features': 2, 'n_heads': 4, 'num_channels': 32, 'num_degrees': 2, 'num_edge_features': 32, 'num_layers': 3}, 'd_hidden': 32, 'd_hidden_templ': 64, 'd_msa': 256, 'd_msa_full': 64, 'd_pair': 128, 'd_templ': 64, 'n_head_msa': 8, 'n_head_pair': 4, 'n_head_templ': 4, 'n_module_2track': 24, 'n_module_3track': 8, 'p_drop': 0.0}

Using CUDA device(s):  cuda:0: (GeForce RTX 2080); 

Parsing input pdb...

Generating sh3_r1_70, length 83...
Forcing amino acids:  B7P,B8P,B9P,B10R,B11P,B12P,B13K,B14P
Loss term           | Weight      
cce                   1.00
entropy               1.00
rep                   2.00
rog                   1.00

Stage 0
Starting gradient descent...
        step    avg loss         cce     entropy         rep         rog
           0      2.1076      1.7515      2.8906      0.0347      5.8264
          10      1.5827      1.6255      2.8108      0.0000      3.4772
          20      0.9264      1.6606      2.8702      0.0135      0.0746
          30      1.7374      1.6391      2.8151      2.0734      0.0861
          40      2.9022      1.6672      2.8042      5.0157      0.0081
          50      1.9827      1.5841      2.6771      2.7998      0.0529
          60      1.5739      1.7483      2.8396      1.6356      0.0106
          70      1.5668      1.7529      2.8472      1.5744      0.0852
          80      0.9401      1.8100      2.8099      0.0247      0.0313
          90      1.0207      1.5927      2.7533      0.0000      0.7574
         100      0.9624      1.6480      2.8811      0.1313      0.0204
         110      1.3525      1.5810      2.8783      1.1255      0.0519
         120      1.1543      1.6620      2.9495      0.5487      0.0628
         130      2.1002      1.5993      2.9421      2.9258      0.1080
         140      1.9006      1.6405      2.8576      2.4882      0.0283
         150      0.9218      1.6441      2.9163      0.0125      0.0234
         160      1.1706      1.6279      2.9191      0.5839      0.1383
         170      1.8297      1.5527      2.9477      2.3157      0.0167
         180      2.7474      1.5755      2.9766      4.5896      0.0057
         190      0.9625      1.5326      2.9610      0.0000      0.3190
         200      0.9725      1.6684      2.8778      0.1508      0.0145
         210      0.9198      1.6024      2.9106      0.0000      0.0861
         220      1.2592      1.5597      2.8817      0.7769      0.3008
         230      1.4641      1.5532      2.8832      1.4140      0.0563
         240      0.9015      1.5906      2.8959      0.0001      0.0208
         250      0.8957      1.6249      2.8393      0.0000      0.0143
         260      0.9723      1.6362      2.7674      0.1994      0.0592
         270      1.0038      1.5821      2.7759      0.3011      0.0586
         280      0.9314      1.5730      2.8209      0.0405      0.1821
         290      0.9044      1.6229      2.8374      0.0001      0.0617
         300      0.8949      1.6022      2.8646      0.0000      0.0075
         310      0.8985      1.5340      2.8054      0.0000      0.1529
         320      0.8644      1.5169      2.7503      0.0001      0.0544
         330      0.9022      1.5329      2.8795      0.0000      0.0988
         340      0.8898      1.5842      2.8523      0.0000      0.0123
         350      0.9245      1.5791      2.8991      0.0021      0.1402
         360      0.8886      1.6289      2.7670      0.0000      0.0472
         370      0.8918      1.5955      2.8539      0.0000      0.0097
         380      0.9135      1.6415      2.8878      0.0000      0.0383
         390      1.5902      1.5955      2.8125      1.7674      0.0081
         400      1.0077      1.5052      2.7680      0.3765      0.0121
         410      0.9308      1.5993      2.7788      0.1332      0.0095
         420      0.9472      1.5589      2.6819      0.2389      0.0176
         430      0.8713      1.5873      2.7573      0.0000      0.0122
         440      0.8730      1.6588      2.6709      0.0006      0.0340
         450      1.0133      1.6145      2.6642      0.3868      0.0144
         460      0.8619      1.5990      2.6873      0.0000      0.0234
         470      0.9077      1.5774      2.7395      0.1007      0.0201
         480      0.8234      1.6178      2.4568      0.0000      0.0423
         490      0.8770      1.6503      2.7127      0.0000      0.0217
         500      0.8479      1.6356      2.5623      0.0006      0.0404
         510      0.8033      1.4919      2.4544      0.0145      0.0413
         520      0.8139      1.5307      2.4967      0.0000      0.0420
         530      0.8087      1.4623      2.5478      0.0002      0.0330
         540      1.3372      1.5723      2.8218      1.1419      0.0082
         550      0.7707      1.4825      2.2748      0.0191      0.0578
         560      0.7982      1.4701      2.4923      0.0000      0.0285
         570      1.3308      1.4852      2.6802      0.0000      2.4884
         580      0.8000      1.4582      2.4061      0.0468      0.0422
         590      0.7758      1.4808      2.3555      0.0017      0.0393
         600      0.8177      1.5058      2.5484      0.0000      0.0342
       final      0.7637      1.3850      2.3842      0.0037      0.0422
best loss step: 585
Max CUDA memory: 1.2488G
Saving /mnt/home/jue/halluc/linear_motifs/output/20220103_sh3_r1/sh3_r1_70: npz, fas, trb, trk, trfold pdb
Finished design /mnt/home/jue/halluc/linear_motifs/output/20220103_sh3_r1/sh3_r1_70 in 15.28 minutes.

Generating sh3_r1_71, length 67...
Forcing amino acids:  B7P,B8P,B9P,B10R,B11P,B12P,B13K,B14P
Loss term           | Weight      
cce                   1.00
entropy               1.00
rep                   2.00
rog                   1.00

Stage 0
Starting gradient descent...
        step    avg loss         cce     entropy         rep         rog
           0      1.3300      1.6356      2.9443      0.0000      2.0702
          10      0.8904      1.5603      2.8413      0.0000      0.0506
          20      1.8121      1.5879      2.7285      2.3697      0.0045
          30      1.0149      1.5623      2.7589      0.3727      0.0080
          40      1.1346      1.6145      2.9275      0.5644      0.0021
          50      0.8467      1.5285      2.6981      0.0000      0.0070
          60      0.7873      1.3699      2.5453      0.0000      0.0211
          70      0.8048      1.4070      2.6051      0.0000      0.0121
          80      0.8375      1.5128      2.6373      0.0135      0.0104
          90      0.7747      1.4360      2.4094      0.0000      0.0280
         100      0.8156      1.4898      2.5593      0.0000      0.0290
         110      0.7807      1.4005      2.4697      0.0000      0.0335
         120      0.7303      1.4375      2.1814      0.0000      0.0327
         130      0.7897      1.5025      2.4177      0.0018      0.0249
         140      1.0703      1.5428      2.3084      0.7442      0.0117
         150      0.8513      1.4933      2.3588      0.1962      0.0119
         160      0.7595      1.5286      2.2566      0.0000      0.0124
         170      0.7243      1.4368      2.1673      0.0006      0.0160
         180      0.7176      1.4785      2.0941      0.0000      0.0151
         190      0.7317      1.4711      2.1676      0.0000      0.0197
         200      0.7627      1.4740      2.2951      0.0155      0.0137
         210      0.7450      1.4613      2.2346      0.0077      0.0138
         220      0.7344      1.4816      2.1717      0.0000      0.0188
         230      0.6954      1.3963      2.0594      0.0021      0.0172
         240      0.7159      1.4502      2.1132      0.0002      0.0157
         250      0.7246      1.4654      2.1420      0.0000      0.0154
         260      0.7514      1.4080      2.3276      0.0027      0.0158
         270      0.7193      1.4541      2.1262      0.0000      0.0164
         280      0.6959      1.3607      2.1021      0.0000      0.0166
         290      0.7643      1.4509      2.2911      0.0338      0.0117
         300      0.7166      1.4738      2.0918      0.0000      0.0172
         310      0.8416      1.4429      2.6925      0.0315      0.0093
         320      0.8021      1.4140      2.5084      0.0000      0.0880
         330      0.7411      1.4829      2.1821      0.0121      0.0163
         340      0.7175      1.4517      2.1182      0.0000      0.0177
         350      0.7112      1.4239      2.1106      0.0000      0.0215
         360      0.7149      1.4592      2.0968      0.0003      0.0178
         370      0.7104      1.4145      2.1196      0.0000      0.0176
         380      0.7271      1.4665      2.1369      0.0076      0.0170
         390      0.7171      1.4612      2.1074      0.0000      0.0168
         400      0.7372      1.4464      2.1788      0.0224      0.0160
         410      0.7896      1.4830      2.4526      0.0000      0.0125
         420      0.7300      1.4333      2.1278      0.0352      0.0186
         430      0.7271      1.4634      2.1503      0.0000      0.0217
         440      0.7555      1.4523      2.3029      0.0000      0.0222
         450      0.7243      1.4414      2.1619      0.0005      0.0173
         460      0.7907      1.4405      2.5022      0.0000      0.0108
         470      0.7834      1.4316      2.4679      0.0000      0.0177
         480      0.7313      1.4300      2.2107      0.0000      0.0156
         490      0.7681      1.4445      2.3634      0.0077      0.0172
         500      0.7469      1.4676      2.2529      0.0000      0.0139
         510      0.7524      1.4507      2.2976      0.0000      0.0136
         520      0.7502      1.5229      2.2095      0.0000      0.0188
         530      0.7711      1.3890      2.3961      0.0291      0.0120
         540      0.7181      1.4971      2.0781      0.0000      0.0151
         550      0.7629      1.4983      2.3036      0.0000      0.0125
         560      0.7236      1.4331      2.1249      0.0222      0.0157
         570      0.7778      1.4853      2.3869      0.0000      0.0167
         580      0.7310      1.4154      2.1653      0.0297      0.0150
         590      0.8757      1.4129      2.4901      0.2316      0.0122
         600      0.7277      1.4475      2.1758      0.0000      0.0153
       final      0.6908      1.4107      2.0268      0.0000      0.0165
best loss step: 362
Max CUDA memory: 0.9109G
Saving /mnt/home/jue/halluc/linear_motifs/output/20220103_sh3_r1/sh3_r1_71: npz, fas, trb, trk, trfold pdb
Finished design /mnt/home/jue/halluc/linear_motifs/output/20220103_sh3_r1/sh3_r1_71 in 14.55 minutes.
corrupted size vs. prev_size while consolidating
